{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cx2313_ly2637 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import urllib.parse\n",
    "\n",
    "import geoalchemy2 as gdb\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from shapely import wkt\n",
    "import sqlalchemy as db\n",
    "\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import Column, Integer, Float, String, DateTime, create_engine\n",
    "from sqlalchemy.ext.declarative import declarative_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ZIPCODE_DATA_FILE = DATA_DIR / \"zipcodes\" / \"ZIP_CODE_040114.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "\n",
    "NYC_DATA_APP_TOKEN = \"4P7xr8685SCdZVFOLXScTCqJi\"\n",
    "BASE_NYC_DATA_URL = \"https://data.cityofnewyork.us/\"\n",
    "NYC_DATA_311 = \"erm2-nwe9.geojson\"\n",
    "NYC_DATA_TREES = \"5rq2-4hqu.geojson\"\n",
    "\n",
    "DB_SCHEMA_FILE = \"schema.sql\"\n",
    "# directory where DB queries for Part 3 will be saved\n",
    "QUERY_DIR = pathlib.Path(\"queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data Preprocessing\n",
    "\n",
    "The first part of the project involves two main activities. The initial step is to download specific datasets manually. This is followed by using Python scripts for automated data downloads. Once the data is collected, the next step is to sort through it. This includes selecting the relevant information, fixing any missing or incorrect data, and creating samples from these datasets for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load and clean data for zipcode file\n",
    "- For the zipcode file, first, we'll remove any columns that are not needed. Next, we'll review the basic information of the dataset. Following that, we'll identify the parts that need cleaning and proceed with the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data and remove unnecessary columns\n",
    "zipcode_data_file = DATA_DIR / \"nyc_zipcodes.shp\"\n",
    "gdf = gpd.read_file(zipcode_data_file)\n",
    "\n",
    "columns_to_keep = ['ZIPCODE', 'STATE', 'COUNTY', 'geometry']\n",
    "zipcode_gdf = gdf[columns_to_keep]\n",
    "\n",
    "# Rename columns for consistency\n",
    "zipcode_gdf = zipcode_gdf.rename(columns={'ZIPCODE': 'zipcode', 'City': 'city','COUNTY': 'county','STATE': 'state'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 263 entries, 0 to 262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   zipcode   263 non-null    object  \n",
      " 1   state     263 non-null    object  \n",
      " 2   county    263 non-null    object  \n",
      " 3   geometry  263 non-null    geometry\n",
      "dtypes: geometry(1), object(3)\n",
      "memory usage: 8.3+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11436</td>\n",
       "      <td>NY</td>\n",
       "      <td>Queens</td>\n",
       "      <td>POLYGON ((1038098.252 188138.380, 1038141.936 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11213</td>\n",
       "      <td>NY</td>\n",
       "      <td>Kings</td>\n",
       "      <td>POLYGON ((1001613.713 186926.440, 1002314.243 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11212</td>\n",
       "      <td>NY</td>\n",
       "      <td>Kings</td>\n",
       "      <td>POLYGON ((1011174.276 183696.338, 1011373.584 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11225</td>\n",
       "      <td>NY</td>\n",
       "      <td>Kings</td>\n",
       "      <td>POLYGON ((995908.365 183617.613, 996522.848 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11218</td>\n",
       "      <td>NY</td>\n",
       "      <td>Kings</td>\n",
       "      <td>POLYGON ((991997.113 176307.496, 992042.798 17...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  zipcode state  county                                           geometry\n",
       "0   11436    NY  Queens  POLYGON ((1038098.252 188138.380, 1038141.936 ...\n",
       "1   11213    NY   Kings  POLYGON ((1001613.713 186926.440, 1002314.243 ...\n",
       "2   11212    NY   Kings  POLYGON ((1011174.276 183696.338, 1011373.584 ...\n",
       "3   11225    NY   Kings  POLYGON ((995908.365 183617.613, 996522.848 18...\n",
       "4   11218    NY   Kings  POLYGON ((991997.113 176307.496, 992042.798 17..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: basic information of the data.\n",
    "print(zipcode_gdf.info())\n",
    "zipcode_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the results we observed, it's clear that the geometry column in the zipcode file is not in the WGS coordinate system. Therefore, we will need to include a transformation in our clean function to convert the geometries to the WGS system for consistency and analysis compatibility.\n",
    "\n",
    "- Furthermore, we will drop any duplicate rows to ensure data integrity and accuracy for our analysis and reporting. This step is crucial for maintaining the quality of our dataset and providing reliable insights.\n",
    "\n",
    "    - In our dataset, each zipcode is intended to correspond to a unique set of boundary data, representing a specific geographic area. This one-to-one relationship between a zipcode and its boundaries is crucial for the integrity and accuracy of our geographical analyses. Therefore, we have opted for a data cleaning approach that involves removing duplicate entries based solely on the `zipcode` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clean function\n",
    "def clean_zipcode_data(gdf):\n",
    "    gdf_cleaned = gdf.copy()\n",
    "    \n",
    "    # Change to WGS system for consistency\n",
    "    gdf_cleaned = gdf_cleaned.to_crs(epsg=4326)\n",
    "    \n",
    "    # Validate geometric data in 'geometry'\n",
    "    gdf_cleaned = gdf_cleaned[gdf_cleaned['geometry'].is_valid]\n",
    "\n",
    "    # Remove rows with any missing values\n",
    "    gdf_cleaned = gdf_cleaned.dropna()\n",
    "    \n",
    "    # Remove duplicates\n",
    "    gdf_cleaned = gdf_cleaned.drop_duplicates(subset='zipcode')\n",
    "\n",
    "    # Check that all zipcodes are 5 characters long\n",
    "    gdf_cleaned = gdf_cleaned[gdf_cleaned['zipcode'].apply(lambda x: len(str(x)) == 5)]\n",
    "\n",
    "    # Ensure categorical consistency\n",
    "    categorical_columns = ['state', 'county']\n",
    "    for col in categorical_columns:\n",
    "        gdf_cleaned.loc[:, col] = gdf_cleaned[col].str.title()\n",
    "    \n",
    "    return gdf_cleaned\n",
    "\n",
    "cleaned_zipcode_gdf = clean_zipcode_data(zipcode_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check cleaned data in details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 248 entries, 0 to 262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   zipcode   248 non-null    object  \n",
      " 1   state     248 non-null    object  \n",
      " 2   county    248 non-null    object  \n",
      " 3   geometry  248 non-null    geometry\n",
      "dtypes: geometry(1), object(3)\n",
      "memory usage: 9.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11436</td>\n",
       "      <td>Ny</td>\n",
       "      <td>Queens</td>\n",
       "      <td>POLYGON ((-73.80585 40.68291, -73.80569 40.682...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11213</td>\n",
       "      <td>Ny</td>\n",
       "      <td>Kings</td>\n",
       "      <td>POLYGON ((-73.93740 40.67973, -73.93487 40.679...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11212</td>\n",
       "      <td>Ny</td>\n",
       "      <td>Kings</td>\n",
       "      <td>POLYGON ((-73.90294 40.67084, -73.90223 40.668...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11225</td>\n",
       "      <td>Ny</td>\n",
       "      <td>Kings</td>\n",
       "      <td>POLYGON ((-73.95797 40.67066, -73.95576 40.670...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11218</td>\n",
       "      <td>Ny</td>\n",
       "      <td>Kings</td>\n",
       "      <td>POLYGON ((-73.97208 40.65060, -73.97192 40.650...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  zipcode state  county                                           geometry\n",
       "0   11436    Ny  Queens  POLYGON ((-73.80585 40.68291, -73.80569 40.682...\n",
       "1   11213    Ny   Kings  POLYGON ((-73.93740 40.67973, -73.93487 40.679...\n",
       "2   11212    Ny   Kings  POLYGON ((-73.90294 40.67084, -73.90223 40.668...\n",
       "3   11225    Ny   Kings  POLYGON ((-73.95797 40.67066, -73.95576 40.670...\n",
       "4   11218    Ny   Kings  POLYGON ((-73.97208 40.65060, -73.97192 40.650..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cleaned_zipcode_gdf.info())\n",
    "cleaned_zipcode_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the zipcode information for New York City, we need to extract this information to use as a reference. This will help us filter the data needed for `311`, `tree`, and `Zillow`. We will include this step in the cleaning steps for `tree`, `311`, and `Zillow` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Download and clean data for trees \n",
    "- For the download part, due to the large volume of data, a script is implemented to download, process, and store New York City tree data in a GeoJSON file, handling the data in chunks to manage the size and complexity efficiently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nyc_geojson_data(url_base, filename, force=False):\n",
    "    limit = 50000  \n",
    "    offset = 0  \n",
    "    all_data = []\n",
    "\n",
    "    if force or not filename.exists():\n",
    "        print(f\"Downloading data to {filename}...\")\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                params = {\n",
    "                    '$limit': limit,\n",
    "                    '$offset': offset,\n",
    "                    '$$app_token': \"4P7xr8685SCdZVFOLXScTCqJi\"\n",
    "                }\n",
    "                url_with_params = f\"{url_base}?{urllib.parse.urlencode(params)}\"\n",
    "                print(\"Requesting URL:\", url_with_params)\n",
    "                response = requests.get(url_with_params)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                data = response.json()\n",
    "                if not data['features']: \n",
    "                    break\n",
    "\n",
    "                all_data.extend(data['features'])\n",
    "                offset += limit\n",
    "            \n",
    "            geojson_feature_collection = {\n",
    "                \"type\": \"FeatureCollection\",\n",
    "                \"features\": all_data\n",
    "            }\n",
    "\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(geojson_feature_collection, f)\n",
    "\n",
    "            print(f\"Done downloading data to {filename}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        print(f\"Reading from {filename}...\")\n",
    "\n",
    "    return filename\n",
    "\n",
    "nyc_tree_data_file = DATA_DIR / \"nyc_tree_data.geojson\"\n",
    "download_nyc_geojson_data(\"https://data.cityofnewyork.us/resource/5rq2-4hqu.geojson\", nyc_tree_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we will use GeoPandas to read the GeoJSON file, and then examine the data for any inconsistencies or areas that may require cleaning.\n",
    "- Upon understanding the issues that require analysis, we will also select the necessary columns in this step, The columns we will use are `tree_id`, `zipcode`, `latitude`, `longitude`, `status`, `health`, `spc_common`, and `geometry`.\n",
    "\n",
    "In summary, regarding the tree data, our approach will be as follows: First, we will remove unnecessary columns. Second, we will examine the basic information of the data. Third, we will clean the data where necessary. Since we are using GeoPandas to read the GeoJSON data, the dataframe will include a column named 'geometry', which is already in the WGS coordinate system, so no further processing is required for this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data and remove unnecessary columns.\n",
    "nyc_tree_data_file = DATA_DIR / \"nyc_tree_data.geojson\"\n",
    "\n",
    "geodf_tree_data=gpd.read_file(nyc_tree_data_file)\n",
    "columns_to_keep = ['tree_id', 'zipcode', 'latitude', 'longitude', 'status', 'health', 'spc_common', 'geometry']\n",
    "tree_gdf = geodf_tree_data[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 683788 entries, 0 to 683787\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count   Dtype   \n",
      "---  ------      --------------   -----   \n",
      " 0   tree_id     683788 non-null  object  \n",
      " 1   zipcode     683788 non-null  object  \n",
      " 2   latitude    683788 non-null  object  \n",
      " 3   longitude   683788 non-null  object  \n",
      " 4   status      683788 non-null  object  \n",
      " 5   health      652172 non-null  object  \n",
      " 6   spc_common  652169 non-null  object  \n",
      " 7   geometry    683788 non-null  geometry\n",
      "dtypes: geometry(1), object(7)\n",
      "memory usage: 41.7+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree_id</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>status</th>\n",
       "      <th>health</th>\n",
       "      <th>spc_common</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180683</td>\n",
       "      <td>11375</td>\n",
       "      <td>40.72309177</td>\n",
       "      <td>-73.84421522</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Fair</td>\n",
       "      <td>red maple</td>\n",
       "      <td>POINT (-73.84422 40.72309)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200540</td>\n",
       "      <td>11357</td>\n",
       "      <td>40.79411067</td>\n",
       "      <td>-73.81867946</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Fair</td>\n",
       "      <td>pin oak</td>\n",
       "      <td>POINT (-73.81868 40.79411)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>204026</td>\n",
       "      <td>11211</td>\n",
       "      <td>40.71758074</td>\n",
       "      <td>-73.9366077</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Good</td>\n",
       "      <td>honeylocust</td>\n",
       "      <td>POINT (-73.93661 40.71758)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204337</td>\n",
       "      <td>11211</td>\n",
       "      <td>40.71353749</td>\n",
       "      <td>-73.93445616</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Good</td>\n",
       "      <td>honeylocust</td>\n",
       "      <td>POINT (-73.93446 40.71354)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189565</td>\n",
       "      <td>11215</td>\n",
       "      <td>40.66677776</td>\n",
       "      <td>-73.97597938</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Good</td>\n",
       "      <td>American linden</td>\n",
       "      <td>POINT (-73.97598 40.66678)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tree_id zipcode     latitude     longitude status health       spc_common  \\\n",
       "0  180683   11375  40.72309177  -73.84421522  Alive   Fair        red maple   \n",
       "1  200540   11357  40.79411067  -73.81867946  Alive   Fair          pin oak   \n",
       "2  204026   11211  40.71758074   -73.9366077  Alive   Good      honeylocust   \n",
       "3  204337   11211  40.71353749  -73.93445616  Alive   Good      honeylocust   \n",
       "4  189565   11215  40.66677776  -73.97597938  Alive   Good  American linden   \n",
       "\n",
       "                     geometry  \n",
       "0  POINT (-73.84422 40.72309)  \n",
       "1  POINT (-73.81868 40.79411)  \n",
       "2  POINT (-73.93661 40.71758)  \n",
       "3  POINT (-73.93446 40.71354)  \n",
       "4  POINT (-73.97598 40.66678)  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: basic information of the data.\n",
    "print(tree_gdf.info())\n",
    "tree_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage with nan: 4.62%\n",
      "Number of duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "tree_with_nan = tree_gdf[tree_gdf.isnull().any(axis=1)]\n",
    "num_rows_with_nan = len(tree_with_nan)\n",
    "total_rows = len(tree_gdf)\n",
    "percent_with_nan = (num_rows_with_nan / total_rows) * 100\n",
    "\n",
    "print(f\"percentage with nan: {percent_with_nan:.2f}%\")\n",
    "print(f\"Number of duplicates: {tree_gdf.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output indicates that the GeoDataFrame contains 683,788 entries across 7 columns. Of these, only the `health` and \n",
    "`spc_common` columns have missing values, as all other columns have a non-null count of 683,788. \n",
    "\n",
    "Given that the `health` and `spc_common` columns cannot be effectively imputed, and after assessing that missing values account for only 4.62% of the total data, the decision to delete all rows with missing values is deemed reasonable. \n",
    "\n",
    "Due to the diverse nature of our tree datasets, it containing unique and valuable information, we have opted to remove duplicates only when an entire row is identical. This approach ensures the preservation of all distinct data points, crucial for our comprehensive analysis.\n",
    "\n",
    "- For example, a single tree_id might have several records that document various states of the same tree over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clean function\n",
    "def clean_tree_data(gdf):\n",
    "    gdf_cleaned = gdf.copy()\n",
    "    \n",
    "    # Filter based on zip codes present in the 'zipcode' dataframe\n",
    "    gdf_cleaned = gdf_cleaned[gdf_cleaned['zipcode'].isin(cleaned_zipcode_gdf['zipcode'])]\n",
    "    \n",
    "    # Convert 'latitude' and 'longitude' to float\n",
    "    gdf_cleaned['latitude'] = pd.to_numeric(gdf_cleaned['latitude'], errors='coerce')\n",
    "    gdf_cleaned['longitude'] = pd.to_numeric(gdf_cleaned['longitude'], errors='coerce')\n",
    "    \n",
    "    # Ensure categorical consistency\n",
    "    categorical_columns = ['status', 'health', 'spc_common']\n",
    "    for col in categorical_columns:\n",
    "        gdf_cleaned.loc[:, col] = gdf_cleaned[col].str.title() \n",
    "    \n",
    "    # Validate geometric data in 'geometry'\n",
    "    gdf_cleaned = gdf_cleaned[gdf_cleaned['geometry'].is_valid]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    gdf_cleaned = gdf_cleaned.drop_duplicates()\n",
    "    \n",
    "    # Remove rows with missing values\n",
    "    gdf_cleaned = gdf_cleaned.dropna()\n",
    "    \n",
    "    return gdf_cleaned\n",
    "\n",
    "cleaned_tree_gdf = clean_tree_data(tree_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check clean data for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 652167 entries, 0 to 683787\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count   Dtype   \n",
      "---  ------      --------------   -----   \n",
      " 0   tree_id     652167 non-null  object  \n",
      " 1   zipcode     652167 non-null  object  \n",
      " 2   latitude    652167 non-null  float64 \n",
      " 3   longitude   652167 non-null  float64 \n",
      " 4   status      652167 non-null  object  \n",
      " 5   health      652167 non-null  object  \n",
      " 6   spc_common  652167 non-null  object  \n",
      " 7   geometry    652167 non-null  geometry\n",
      "dtypes: float64(2), geometry(1), object(5)\n",
      "memory usage: 44.8+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree_id</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>status</th>\n",
       "      <th>health</th>\n",
       "      <th>spc_common</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180683</td>\n",
       "      <td>11375</td>\n",
       "      <td>40.723092</td>\n",
       "      <td>-73.844215</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Fair</td>\n",
       "      <td>Red Maple</td>\n",
       "      <td>POINT (-73.84422 40.72309)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200540</td>\n",
       "      <td>11357</td>\n",
       "      <td>40.794111</td>\n",
       "      <td>-73.818679</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Fair</td>\n",
       "      <td>Pin Oak</td>\n",
       "      <td>POINT (-73.81868 40.79411)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>204026</td>\n",
       "      <td>11211</td>\n",
       "      <td>40.717581</td>\n",
       "      <td>-73.936608</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Good</td>\n",
       "      <td>Honeylocust</td>\n",
       "      <td>POINT (-73.93661 40.71758)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204337</td>\n",
       "      <td>11211</td>\n",
       "      <td>40.713537</td>\n",
       "      <td>-73.934456</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Good</td>\n",
       "      <td>Honeylocust</td>\n",
       "      <td>POINT (-73.93446 40.71354)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189565</td>\n",
       "      <td>11215</td>\n",
       "      <td>40.666778</td>\n",
       "      <td>-73.975979</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Good</td>\n",
       "      <td>American Linden</td>\n",
       "      <td>POINT (-73.97598 40.66678)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tree_id zipcode   latitude  longitude status health       spc_common  \\\n",
       "0  180683   11375  40.723092 -73.844215  Alive   Fair        Red Maple   \n",
       "1  200540   11357  40.794111 -73.818679  Alive   Fair          Pin Oak   \n",
       "2  204026   11211  40.717581 -73.936608  Alive   Good      Honeylocust   \n",
       "3  204337   11211  40.713537 -73.934456  Alive   Good      Honeylocust   \n",
       "4  189565   11215  40.666778 -73.975979  Alive   Good  American Linden   \n",
       "\n",
       "                     geometry  \n",
       "0  POINT (-73.84422 40.72309)  \n",
       "1  POINT (-73.81868 40.79411)  \n",
       "2  POINT (-73.93661 40.71758)  \n",
       "3  POINT (-73.93446 40.71354)  \n",
       "4  POINT (-73.97598 40.66678)  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cleaned_tree_gdf.info())\n",
    "cleaned_tree_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Download and clean data for 311 complaints\n",
    "- Regarding the 311 data, given its immense volume, we opted to filter the data during the downloading phase, after understanding the requirements for subsequent analysis. Initially, we filtered the data by date, selecting records between January 1, 2015, at 00:00:00 and September 30, 2023, at 23:59:59. Subsequently, we chose to download only the following columns: `latitude, longitude, incident_zip, created_date, location, complaint_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nyc311_geojson_data(url_base, filename, force=False):\n",
    "    start_date = '2015-01-01T00:00:00'\n",
    "    end_date = '2023-09-30T23:59:59'\n",
    "    limit = 2500000  \n",
    "    offset = 0 \n",
    "    all_data = []\n",
    "\n",
    "    if force or not filename.exists():\n",
    "        print(f\"Downloading data to {filename}...\")\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                params = {\n",
    "                    '$where': f\"created_date between '{start_date}' and '{end_date}'\",\n",
    "                    '$limit': limit,\n",
    "                    '$offset': offset,\n",
    "                    '$$app_token': \"4P7xr8685SCdZVFOLXScTCqJi\",\n",
    "                    '$select': 'latitude,longitude,incident_zip,created_date,location,complaint_type'  \n",
    "                }\n",
    "                url_with_params = f\"{url_base}?{urllib.parse.urlencode(params)}\"\n",
    "                print(\"Requesting URL:\", url_with_params)\n",
    "                response = requests.get(url_with_params)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                data = response.json()\n",
    "                if not data['features']: \n",
    "                    break\n",
    "\n",
    "                all_data.extend(data['features'])  \n",
    "                offset += limit\n",
    "\n",
    "            geojson_feature_collection = {\n",
    "                \"type\": \"FeatureCollection\",\n",
    "                \"features\": all_data\n",
    "            }\n",
    "\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(geojson_feature_collection, f)\n",
    "\n",
    "            print(f\"Done downloading data to {filename}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        print(f\"Reading from {filename}...\")\n",
    "\n",
    "    return filename\n",
    "\n",
    "nyc_311_data_file = DATA_DIR / \"nyc_311_data.geojson\"\n",
    "download_nyc311_geojson_data(\"https://data.cityofnewyork.us/resource/erm2-nwe9.geojson\", nyc_311_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we will follow the same steps as we did when cleaning the tree data. First, we need to review the basic information of the data. Second, we need to identify which parts of the data require cleaning, and then write a clean function to address these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data\n",
    "nyc_311_data_file = DATA_DIR / \"nyc_311_data.geojson\"\n",
    "\n",
    "nyc311_gdf=gpd.read_file(nyc_311_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 24336507 entries, 0 to 24336506\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Dtype         \n",
      "---  ------          -----         \n",
      " 0   latitude        object        \n",
      " 1   created_date    datetime64[ns]\n",
      " 2   longitude       object        \n",
      " 3   incident_zip    object        \n",
      " 4   complaint_type  object        \n",
      " 5   geometry        geometry      \n",
      "dtypes: datetime64[ns](1), geometry(1), object(4)\n",
      "memory usage: 1.1+ GB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>created_date</th>\n",
       "      <th>longitude</th>\n",
       "      <th>incident_zip</th>\n",
       "      <th>complaint_type</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.655672001198894</td>\n",
       "      <td>2023-09-30 23:59:58</td>\n",
       "      <td>-73.95917686020623</td>\n",
       "      <td>11226</td>\n",
       "      <td>Noise - Street/Sidewalk</td>\n",
       "      <td>POINT (-73.95918 40.65567)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.76675595839554</td>\n",
       "      <td>2023-09-30 23:59:38</td>\n",
       "      <td>-73.78751847563191</td>\n",
       "      <td>11361</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>POINT (-73.78752 40.76676)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.71949965458691</td>\n",
       "      <td>2023-09-30 23:59:35</td>\n",
       "      <td>-73.98486650733275</td>\n",
       "      <td>10002</td>\n",
       "      <td>Noise - Commercial</td>\n",
       "      <td>POINT (-73.98487 40.71950)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.687499303408536</td>\n",
       "      <td>2023-09-30 23:59:34</td>\n",
       "      <td>-73.7972903094197</td>\n",
       "      <td>11435</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>POINT (-73.79729 40.68750)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.65220215349917</td>\n",
       "      <td>2023-09-30 23:59:28</td>\n",
       "      <td>-73.9579464603267</td>\n",
       "      <td>11226</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>POINT (-73.95795 40.65220)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             latitude        created_date           longitude incident_zip  \\\n",
       "0  40.655672001198894 2023-09-30 23:59:58  -73.95917686020623        11226   \n",
       "1   40.76675595839554 2023-09-30 23:59:38  -73.78751847563191        11361   \n",
       "2   40.71949965458691 2023-09-30 23:59:35  -73.98486650733275        10002   \n",
       "3  40.687499303408536 2023-09-30 23:59:34   -73.7972903094197        11435   \n",
       "4   40.65220215349917 2023-09-30 23:59:28   -73.9579464603267        11226   \n",
       "\n",
       "            complaint_type                    geometry  \n",
       "0  Noise - Street/Sidewalk  POINT (-73.95918 40.65567)  \n",
       "1      Noise - Residential  POINT (-73.78752 40.76676)  \n",
       "2       Noise - Commercial  POINT (-73.98487 40.71950)  \n",
       "3      Noise - Residential  POINT (-73.79729 40.68750)  \n",
       "4      Noise - Residential  POINT (-73.95795 40.65220)  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: basic information of the data.\n",
    "print(nyc311_gdf.info())\n",
    "nyc311_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage with nan: 5.37%\n",
      "Number of duplicates: 907072\n"
     ]
    }
   ],
   "source": [
    "nyc311_with_nan = nyc311_gdf[nyc311_gdf.isnull().any(axis=1)]\n",
    "num_rows_with_nan = len(nyc311_with_nan)\n",
    "total_rows = len(nyc311_gdf)\n",
    "percent_with_nan = (num_rows_with_nan / total_rows) * 100\n",
    "\n",
    "print(f\"percentage with nan: {percent_with_nan:.2f}%\")\n",
    "print(f\"Number of duplicates: {nyc311_gdf.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['latitude', 'longitude', 'incident_zip', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "# Print columns with missing values\n",
    "columns_with_missing_values = nyc311_gdf.columns[nyc311_gdf.isnull().any()].tolist()\n",
    "print(columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output indicates that the GeoDataFrame contains 24336507 entries across 6 columns. Of these, `latitude`, `longtitude` and \n",
    "`incident_zip` columns have missing values.\n",
    "\n",
    "Additionally, we can observe that there are a total of 907072 identical rows. These will be removed in the subsequent cleaning steps.\n",
    "\n",
    "- When two rows in the dataset are completely identical across all their fields, it typically indicates a case of duplication. Such duplicates don't add any additional value or information. In fact, they can skew analysis and results if not addressed. Therefore, in the upcoming data cleaning steps, one of each pair of identical rows will be removed. \n",
    "\n",
    "Given that the `latitude`, `longtitude` and `incident_zip` columns cannot be effectively imputed, and after assessing that missing values account for only 5.37% of the total data, the decision to delete all rows with missing values is deemed reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clean function\n",
    "def clean_311_data(gdf):\n",
    "    gdf_cleaned = gdf.copy()\n",
    "    \n",
    "    # Rename 'incident_zip' to 'zipcode' for consistant in later mapping\n",
    "    gdf_cleaned.rename(columns={'incident_zip': 'zipcode'}, inplace=True)\n",
    "    \n",
    "    # Filter based on zip codes present in the 'zipcode' dataframe\n",
    "    gdf_cleaned = gdf_cleaned[gdf_cleaned['zipcode'].isin(cleaned_zipcode_gdf['zipcode'])]\n",
    "    \n",
    "    # Convert 'latitude' and 'longitude' to float\n",
    "    gdf_cleaned['latitude'] = pd.to_numeric(gdf_cleaned['latitude'], errors='coerce')\n",
    "    gdf_cleaned['longitude'] = pd.to_numeric(gdf_cleaned['longitude'], errors='coerce')\n",
    "    \n",
    "    # Validate geometric data in 'geometry'\n",
    "    gdf_cleaned = gdf_cleaned[gdf_cleaned['geometry'].is_valid]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    gdf_cleaned = gdf_cleaned.drop_duplicates()\n",
    "    \n",
    "    # Remove rows with missing values\n",
    "    gdf_cleaned = gdf_cleaned.dropna(subset=['latitude', 'longitude', 'zipcode'])\n",
    "    \n",
    "    return gdf_cleaned\n",
    "\n",
    "cleaned_311_gdf = clean_311_data(nyc311_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "Int64Index: 22233352 entries, 0 to 24336505\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Dtype         \n",
      "---  ------          -----         \n",
      " 0   latitude        float64       \n",
      " 1   created_date    datetime64[ns]\n",
      " 2   longitude       float64       \n",
      " 3   zipcode         object        \n",
      " 4   complaint_type  object        \n",
      " 5   geometry        geometry      \n",
      "dtypes: datetime64[ns](1), float64(2), geometry(1), object(2)\n",
      "memory usage: 1.2+ GB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>created_date</th>\n",
       "      <th>longitude</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>complaint_type</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.655672</td>\n",
       "      <td>2023-09-30 23:59:58</td>\n",
       "      <td>-73.959177</td>\n",
       "      <td>11226</td>\n",
       "      <td>Noise - Street/Sidewalk</td>\n",
       "      <td>POINT (-73.95918 40.65567)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.766756</td>\n",
       "      <td>2023-09-30 23:59:38</td>\n",
       "      <td>-73.787518</td>\n",
       "      <td>11361</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>POINT (-73.78752 40.76676)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.719500</td>\n",
       "      <td>2023-09-30 23:59:35</td>\n",
       "      <td>-73.984867</td>\n",
       "      <td>10002</td>\n",
       "      <td>Noise - Commercial</td>\n",
       "      <td>POINT (-73.98487 40.71950)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.687499</td>\n",
       "      <td>2023-09-30 23:59:34</td>\n",
       "      <td>-73.797290</td>\n",
       "      <td>11435</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>POINT (-73.79729 40.68750)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.652202</td>\n",
       "      <td>2023-09-30 23:59:28</td>\n",
       "      <td>-73.957946</td>\n",
       "      <td>11226</td>\n",
       "      <td>Noise - Residential</td>\n",
       "      <td>POINT (-73.95795 40.65220)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude        created_date  longitude zipcode           complaint_type  \\\n",
       "0  40.655672 2023-09-30 23:59:58 -73.959177   11226  Noise - Street/Sidewalk   \n",
       "1  40.766756 2023-09-30 23:59:38 -73.787518   11361      Noise - Residential   \n",
       "2  40.719500 2023-09-30 23:59:35 -73.984867   10002       Noise - Commercial   \n",
       "3  40.687499 2023-09-30 23:59:34 -73.797290   11435      Noise - Residential   \n",
       "4  40.652202 2023-09-30 23:59:28 -73.957946   11226      Noise - Residential   \n",
       "\n",
       "                     geometry  \n",
       "0  POINT (-73.95918 40.65567)  \n",
       "1  POINT (-73.78752 40.76676)  \n",
       "2  POINT (-73.98487 40.71950)  \n",
       "3  POINT (-73.79729 40.68750)  \n",
       "4  POINT (-73.95795 40.65220)  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cleaned_311_gdf.info())\n",
    "cleaned_311_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Load and clean data for Historical monthly average rents by zip code from Zillow\n",
    "- For the Zillow data, since our entire project is focused on New York City, we will first need to filter for data pertaining to NYC. Then, based on our understanding of the requirements for subsequent analysis, we will retain the columns that are useful.\n",
    "\n",
    "- According to the subsequent analysis requirements, we only need to reserve the rental prices of 2023-08-31, 2023-01-31 and 2023-09-31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data and remove unnecessary columns\n",
    "zillow_data_file = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "df = pd.read_csv(zillow_data_file)\n",
    "\n",
    "columns_to_keep = ['2023-08-31','2023-01-31','2023-09-30','RegionName','City','CountyName']\n",
    "zillow_df = df[columns_to_keep]\n",
    "\n",
    "# Rename columns for consistency\n",
    "zillow_df = zillow_df.rename(columns={'RegionName': 'zipcode','City': 'city','CountyName': 'county'})\n",
    "zillow_df = zillow_df.rename(columns = {'2023-08-31':'date_2023_08_31','2023-01-31':'date_2023_01_31','2023-09-30':'date_2023_09_30'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6722 entries, 0 to 6721\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   date_2023_08_31  6251 non-null   float64\n",
      " 1   date_2023_01_31  4816 non-null   float64\n",
      " 2   date_2023_09_30  6722 non-null   float64\n",
      " 3   zipcode          6722 non-null   int64  \n",
      " 4   city             6665 non-null   object \n",
      " 5   county           6722 non-null   object \n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 315.2+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_2023_08_31</th>\n",
       "      <th>date_2023_01_31</th>\n",
       "      <th>date_2023_09_30</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>city</th>\n",
       "      <th>county</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2053.486247</td>\n",
       "      <td>2027.438438</td>\n",
       "      <td>2055.771355</td>\n",
       "      <td>77494</td>\n",
       "      <td>Katy</td>\n",
       "      <td>Fort Bend County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1795.384582</td>\n",
       "      <td>1738.217986</td>\n",
       "      <td>1799.631140</td>\n",
       "      <td>77449</td>\n",
       "      <td>Katy</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1757.602011</td>\n",
       "      <td>1706.900064</td>\n",
       "      <td>1755.031490</td>\n",
       "      <td>77084</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Harris County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1488.180414</td>\n",
       "      <td>1458.063897</td>\n",
       "      <td>1494.366097</td>\n",
       "      <td>79936</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>El Paso County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3064.476503</td>\n",
       "      <td>2895.699421</td>\n",
       "      <td>3079.585783</td>\n",
       "      <td>11385</td>\n",
       "      <td>New York</td>\n",
       "      <td>Queens County</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_2023_08_31  date_2023_01_31  date_2023_09_30  zipcode      city  \\\n",
       "0      2053.486247      2027.438438      2055.771355    77494      Katy   \n",
       "1      1795.384582      1738.217986      1799.631140    77449      Katy   \n",
       "2      1757.602011      1706.900064      1755.031490    77084   Houston   \n",
       "3      1488.180414      1458.063897      1494.366097    79936   El Paso   \n",
       "4      3064.476503      2895.699421      3079.585783    11385  New York   \n",
       "\n",
       "             county  \n",
       "0  Fort Bend County  \n",
       "1     Harris County  \n",
       "2     Harris County  \n",
       "3    El Paso County  \n",
       "4     Queens County  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: basic information of the data.\n",
    "print(zillow_df.info())\n",
    "zillow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the output above, We see many missing rent prices in the data, so we need to fill these in. This will help us get a full and accurate picture of rent changes over time.\n",
    "\n",
    "- We decided to estimate the missing monthly rents by carrying forward the last recorded price and then using the following known price when necessary. This makes sense because rent doesn't usually change too much from month to month. If we have a missing price, it's a safe bet that it was close to what it was the month before, or it will be similar to the price after. That's why we use these filling methods; they're like educated guesses that keep things simple and sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clean function \n",
    "def clean_zillow_data(df):\n",
    "    # Convert 'zipcode' to an object for consistency with other datasets\n",
    "    df['zipcode'] = df['zipcode'].astype(str)\n",
    "    \n",
    "    # Filter based on zip codes present in the 'zipcode' dataframe\n",
    "    df = df[df['zipcode'].isin(cleaned_zipcode_gdf['zipcode'])]\n",
    "    \n",
    "    # Fill missing data row-wise using forward-fill and then backward-fill\n",
    "    df = df.fillna(method='ffill', axis=0)\n",
    "    df = df.fillna(method='bfill', axis=0)\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # Ensure categorical consistency\n",
    "    categorical_columns = ['city', 'county']\n",
    "    for col in categorical_columns:\n",
    "        df.loc[:, col] = df[col].str.title() \n",
    "    \n",
    "    return df\n",
    "\n",
    "cleaned_zillow_df = clean_zillow_data(zillow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 146 entries, 4 to 6721\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   date_2023_08_31  146 non-null    float64\n",
      " 1   date_2023_01_31  146 non-null    float64\n",
      " 2   date_2023_09_30  146 non-null    float64\n",
      " 3   zipcode          146 non-null    object \n",
      " 4   city             146 non-null    object \n",
      " 5   county           146 non-null    object \n",
      "dtypes: float64(3), object(3)\n",
      "memory usage: 8.0+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_2023_08_31</th>\n",
       "      <th>date_2023_01_31</th>\n",
       "      <th>date_2023_09_30</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>city</th>\n",
       "      <th>county</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3064.476503</td>\n",
       "      <td>2895.699421</td>\n",
       "      <td>3079.585783</td>\n",
       "      <td>11385</td>\n",
       "      <td>New York</td>\n",
       "      <td>Queens County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2737.547470</td>\n",
       "      <td>2588.030194</td>\n",
       "      <td>2728.733333</td>\n",
       "      <td>11208</td>\n",
       "      <td>New York</td>\n",
       "      <td>Kings County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2285.460026</td>\n",
       "      <td>2588.030194</td>\n",
       "      <td>2362.500000</td>\n",
       "      <td>11236</td>\n",
       "      <td>New York</td>\n",
       "      <td>Kings County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2353.686402</td>\n",
       "      <td>2155.617718</td>\n",
       "      <td>2423.888889</td>\n",
       "      <td>10467</td>\n",
       "      <td>New York</td>\n",
       "      <td>Bronx County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2302.557354</td>\n",
       "      <td>2255.604528</td>\n",
       "      <td>2292.994444</td>\n",
       "      <td>11373</td>\n",
       "      <td>New York</td>\n",
       "      <td>Queens County</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    date_2023_08_31  date_2023_01_31  date_2023_09_30 zipcode      city  \\\n",
       "4       3064.476503      2895.699421      3079.585783   11385  New York   \n",
       "6       2737.547470      2588.030194      2728.733333   11208  New York   \n",
       "12      2285.460026      2588.030194      2362.500000   11236  New York   \n",
       "13      2353.686402      2155.617718      2423.888889   10467  New York   \n",
       "14      2302.557354      2255.604528      2292.994444   11373  New York   \n",
       "\n",
       "           county  \n",
       "4   Queens County  \n",
       "6    Kings County  \n",
       "12   Kings County  \n",
       "13   Bronx County  \n",
       "14  Queens County  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cleaned_zillow_df.info())\n",
    "cleaned_zillow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we have four cleaned data:**\n",
    "- cleaned_tree_gdf\n",
    "- cleaned_311_gdf\n",
    "- cleaned_zillow_df\n",
    "- cleaned_zipcode_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First step we have to create the data base and postgis extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createdb: error: database creation failed: ERROR:  database \"FinalProject\" already exists\n",
      "ERROR:  extension \"postgis\" already exists\n"
     ]
    }
   ],
   "source": [
    "! /Applications/Postgres.app/Contents/Versions/latest/bin/createdb 'FinalProject'\n",
    "! /Applications/Postgres.app/Contents/Versions/latest/bin/psql --dbname 'FinalProject' -c 'CREATE EXTENSION postgis;'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this part, we've set up a database using SQLAlchemy and PostgreSQL to manage data related to trees, 311 service requests, zipcode and Zillow. We first created classes. After defining these classes, We generated the corresponding tables in the database. We also created a function for data convertion which is from dataframe to sql. Finally, we saved the SQL schema to a file named `schema.sql` for future reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(psycopg2.OperationalError) \n(Background on this error at: https://sqlalche.me/e/14/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3280\u001b[0m, in \u001b[0;36mEngine._wrap_pool_connect\u001b[1;34m(self, fn, connection)\u001b[0m\n\u001b[0;32m   3279\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3280\u001b[0m     \u001b[39mreturn\u001b[39;00m fn()\n\u001b[0;32m   3281\u001b[0m \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mdbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:310\u001b[0m, in \u001b[0;36mPool.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \n\u001b[0;32m    305\u001b[0m \u001b[39mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \n\u001b[0;32m    309\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m \u001b[39mreturn\u001b[39;00m _ConnectionFairy\u001b[39m.\u001b[39m_checkout(\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:868\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[1;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fairy:\n\u001b[1;32m--> 868\u001b[0m     fairy \u001b[39m=\u001b[39m _ConnectionRecord\u001b[39m.\u001b[39mcheckout(pool)\n\u001b[0;32m    870\u001b[0m     fairy\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m pool\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:476\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[1;34m(cls, pool)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    475\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheckout\u001b[39m(\u001b[39mcls\u001b[39m, pool):\n\u001b[1;32m--> 476\u001b[0m     rec \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39m_do_get()\n\u001b[0;32m    477\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:145\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[0;32m    146\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dec_overflow()\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:70\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarn_only:\n\u001b[1;32m---> 70\u001b[0m         compat\u001b[39m.\u001b[39mraise_(\n\u001b[0;32m     71\u001b[0m             exc_value,\n\u001b[0;32m     72\u001b[0m             with_traceback\u001b[39m=\u001b[39mexc_tb,\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\compat.py:208\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n\u001b[0;32m    209\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[39m# credit to\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[39m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[39m# as the __traceback__ object creates a cycle\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:143\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection()\n\u001b[0;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:256\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m \u001b[39mreturn\u001b[39;00m _ConnectionRecord(\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:371\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[1;34m(self, pool, connect)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[39mif\u001b[39;00m connect:\n\u001b[1;32m--> 371\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__connect()\n\u001b[0;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalize_callback \u001b[39m=\u001b[39m deque()\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:665\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 665\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[0;32m    666\u001b[0m         pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mError on connect(): \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, e)\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:70\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarn_only:\n\u001b[1;32m---> 70\u001b[0m         compat\u001b[39m.\u001b[39mraise_(\n\u001b[0;32m     71\u001b[0m             exc_value,\n\u001b[0;32m     72\u001b[0m             with_traceback\u001b[39m=\u001b[39mexc_tb,\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\compat.py:208\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n\u001b[0;32m    209\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[39m# credit to\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[39m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[39m# as the __traceback__ object creates a cycle\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:661\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarttime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbapi_connection \u001b[39m=\u001b[39m connection \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39m_invoke_creator(\u001b[39mself\u001b[39m)\n\u001b[0;32m    662\u001b[0m pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreated new connection \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, connection)\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:590\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[1;34m(connection_record)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[39mreturn\u001b[39;00m connection\n\u001b[1;32m--> 590\u001b[0m \u001b[39mreturn\u001b[39;00m dialect\u001b[39m.\u001b[39mconnect(\u001b[39m*\u001b[39mcargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams)\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:597\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[1;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams):\n\u001b[0;32m    596\u001b[0m     \u001b[39m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbapi\u001b[39m.\u001b[39mconnect(\u001b[39m*\u001b[39mcargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams)\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\psycopg2\\__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 122\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39mconnection_factory, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwasync)\n\u001b[0;32m    123\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mOperationalError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\allen\\FinalProject4501-1\\cx2313_ly2637_FinalProject.ipynb Cell 45\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/allen/FinalProject4501-1/cx2313_ly2637_FinalProject.ipynb#X63sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     date_2023_01_31 \u001b[39m=\u001b[39m Column(Float)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/allen/FinalProject4501-1/cx2313_ly2637_FinalProject.ipynb#X63sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     date_2023_09_30 \u001b[39m=\u001b[39m Column(Float)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/allen/FinalProject4501-1/cx2313_ly2637_FinalProject.ipynb#X63sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m Base\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mdrop_all(engine)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/allen/FinalProject4501-1/cx2313_ly2637_FinalProject.ipynb#X63sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m Base\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mcreate_all(engine)\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\sql\\schema.py:4945\u001b[0m, in \u001b[0;36mMetaData.drop_all\u001b[1;34m(self, bind, tables, checkfirst)\u001b[0m\n\u001b[0;32m   4943\u001b[0m \u001b[39mif\u001b[39;00m bind \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   4944\u001b[0m     bind \u001b[39m=\u001b[39m _bind_or_error(\u001b[39mself\u001b[39m)\n\u001b[1;32m-> 4945\u001b[0m bind\u001b[39m.\u001b[39m_run_ddl_visitor(\n\u001b[0;32m   4946\u001b[0m     ddl\u001b[39m.\u001b[39mSchemaDropper, \u001b[39mself\u001b[39m, checkfirst\u001b[39m=\u001b[39mcheckfirst, tables\u001b[39m=\u001b[39mtables\n\u001b[0;32m   4947\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3146\u001b[0m, in \u001b[0;36mEngine._run_ddl_visitor\u001b[1;34m(self, visitorcallable, element, **kwargs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_ddl_visitor\u001b[39m(\u001b[39mself\u001b[39m, visitorcallable, element, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 3146\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbegin() \u001b[39mas\u001b[39;00m conn:\n\u001b[0;32m   3147\u001b[0m         conn\u001b[39m.\u001b[39m_run_ddl_visitor(visitorcallable, element, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3062\u001b[0m, in \u001b[0;36mEngine.begin\u001b[1;34m(self, close_with_result)\u001b[0m\n\u001b[0;32m   3060\u001b[0m     conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnect()\n\u001b[0;32m   3061\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3062\u001b[0m     conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnect(close_with_result\u001b[39m=\u001b[39mclose_with_result)\n\u001b[0;32m   3063\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   3064\u001b[0m     trans \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mbegin()\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3234\u001b[0m, in \u001b[0;36mEngine.connect\u001b[1;34m(self, close_with_result)\u001b[0m\n\u001b[0;32m   3219\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m, close_with_result\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   3220\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[0;32m   3221\u001b[0m \n\u001b[0;32m   3222\u001b[0m \u001b[39m    The :class:`_engine.Connection` object is a facade that uses a DBAPI\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3231\u001b[0m \n\u001b[0;32m   3232\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3234\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection_cls(\u001b[39mself\u001b[39m, close_with_result\u001b[39m=\u001b[39mclose_with_result)\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:96\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, engine, connection, close_with_result, _branch_from, _execution_options, _dispatch, _has_events, _allow_revalidate)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_events \u001b[39m=\u001b[39m _branch_from\u001b[39m.\u001b[39m_has_events\n\u001b[0;32m     92\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbapi_connection \u001b[39m=\u001b[39m (\n\u001b[0;32m     94\u001b[0m         connection\n\u001b[0;32m     95\u001b[0m         \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m         \u001b[39melse\u001b[39;00m engine\u001b[39m.\u001b[39mraw_connection()\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transaction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nested_transaction \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__savepoint_seq \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3313\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[1;34m(self, _connection)\u001b[0m\n\u001b[0;32m   3291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraw_connection\u001b[39m(\u001b[39mself\u001b[39m, _connection\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   3292\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[0;32m   3293\u001b[0m \n\u001b[0;32m   3294\u001b[0m \u001b[39m    The returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3311\u001b[0m \n\u001b[0;32m   3312\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_pool_connect(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mconnect, _connection)\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3283\u001b[0m, in \u001b[0;36mEngine._wrap_pool_connect\u001b[1;34m(self, fn, connection)\u001b[0m\n\u001b[0;32m   3281\u001b[0m \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mdbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   3282\u001b[0m     \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 3283\u001b[0m         Connection\u001b[39m.\u001b[39m_handle_dbapi_exception_noconnection(\n\u001b[0;32m   3284\u001b[0m             e, dialect, \u001b[39mself\u001b[39m\n\u001b[0;32m   3285\u001b[0m         )\n\u001b[0;32m   3286\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3287\u001b[0m         util\u001b[39m.\u001b[39mraise_(\n\u001b[0;32m   3288\u001b[0m             sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m1\u001b[39m], with_traceback\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m]\n\u001b[0;32m   3289\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2117\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception_noconnection\u001b[1;34m(cls, e, dialect, engine)\u001b[0m\n\u001b[0;32m   2115\u001b[0m     util\u001b[39m.\u001b[39mraise_(newraise, with_traceback\u001b[39m=\u001b[39mexc_info[\u001b[39m2\u001b[39m], from_\u001b[39m=\u001b[39me)\n\u001b[0;32m   2116\u001b[0m \u001b[39melif\u001b[39;00m should_wrap:\n\u001b[1;32m-> 2117\u001b[0m     util\u001b[39m.\u001b[39mraise_(\n\u001b[0;32m   2118\u001b[0m         sqlalchemy_exception, with_traceback\u001b[39m=\u001b[39mexc_info[\u001b[39m2\u001b[39m], from_\u001b[39m=\u001b[39me\n\u001b[0;32m   2119\u001b[0m     )\n\u001b[0;32m   2120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2121\u001b[0m     util\u001b[39m.\u001b[39mraise_(exc_info[\u001b[39m1\u001b[39m], with_traceback\u001b[39m=\u001b[39mexc_info[\u001b[39m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\compat.py:208\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    205\u001b[0m     exception\u001b[39m.\u001b[39m__cause__ \u001b[39m=\u001b[39m replace_context\n\u001b[0;32m    207\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n\u001b[0;32m    209\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[39m# credit to\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[39m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[39m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[39mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3280\u001b[0m, in \u001b[0;36mEngine._wrap_pool_connect\u001b[1;34m(self, fn, connection)\u001b[0m\n\u001b[0;32m   3278\u001b[0m dialect \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdialect\n\u001b[0;32m   3279\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3280\u001b[0m     \u001b[39mreturn\u001b[39;00m fn()\n\u001b[0;32m   3281\u001b[0m \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mdbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   3282\u001b[0m     \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:310\u001b[0m, in \u001b[0;36mPool.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    303\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \n\u001b[0;32m    305\u001b[0m \u001b[39m    The connection is instrumented such that when its\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \n\u001b[0;32m    309\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m _ConnectionFairy\u001b[39m.\u001b[39m_checkout(\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:868\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[1;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    866\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_checkout\u001b[39m(\u001b[39mcls\u001b[39m, pool, threadconns\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, fairy\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    867\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fairy:\n\u001b[1;32m--> 868\u001b[0m         fairy \u001b[39m=\u001b[39m _ConnectionRecord\u001b[39m.\u001b[39mcheckout(pool)\n\u001b[0;32m    870\u001b[0m         fairy\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m pool\n\u001b[0;32m    871\u001b[0m         fairy\u001b[39m.\u001b[39m_counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:476\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[1;34m(cls, pool)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    475\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheckout\u001b[39m(\u001b[39mcls\u001b[39m, pool):\n\u001b[1;32m--> 476\u001b[0m     rec \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39m_do_get()\n\u001b[0;32m    477\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         dbapi_connection \u001b[39m=\u001b[39m rec\u001b[39m.\u001b[39mget_connection()\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:145\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection()\n\u001b[0;32m    144\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m         \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[0;32m    146\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dec_overflow()\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:70\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarn_only:\n\u001b[1;32m---> 70\u001b[0m         compat\u001b[39m.\u001b[39mraise_(\n\u001b[0;32m     71\u001b[0m             exc_value,\n\u001b[0;32m     72\u001b[0m             with_traceback\u001b[39m=\u001b[39mexc_tb,\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m compat\u001b[39m.\u001b[39mpy3k \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info[\u001b[39m1\u001b[39m]:\n\u001b[0;32m     76\u001b[0m         \u001b[39m# emulate Py3K's behavior of telling us when an exception\u001b[39;00m\n\u001b[0;32m     77\u001b[0m         \u001b[39m# occurs in an exception handler.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\compat.py:208\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    205\u001b[0m     exception\u001b[39m.\u001b[39m__cause__ \u001b[39m=\u001b[39m replace_context\n\u001b[0;32m    207\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n\u001b[0;32m    209\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[39m# credit to\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[39m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[39m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[39mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:143\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inc_overflow():\n\u001b[0;32m    142\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection()\n\u001b[0;32m    144\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m         \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:256\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    254\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     \u001b[39mreturn\u001b[39;00m _ConnectionRecord(\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:371\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[1;34m(self, pool, connect)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__pool \u001b[39m=\u001b[39m pool\n\u001b[0;32m    370\u001b[0m \u001b[39mif\u001b[39;00m connect:\n\u001b[1;32m--> 371\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__connect()\n\u001b[0;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalize_callback \u001b[39m=\u001b[39m deque()\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:665\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfresh \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 665\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[0;32m    666\u001b[0m         pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mError on connect(): \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m    667\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[39m# in SQLAlchemy 1.4 the first_connect event is not used by\u001b[39;00m\n\u001b[0;32m    669\u001b[0m     \u001b[39m# the engine, so this will usually not be set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:70\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarn_only:\n\u001b[1;32m---> 70\u001b[0m         compat\u001b[39m.\u001b[39mraise_(\n\u001b[0;32m     71\u001b[0m             exc_value,\n\u001b[0;32m     72\u001b[0m             with_traceback\u001b[39m=\u001b[39mexc_tb,\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m compat\u001b[39m.\u001b[39mpy3k \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info[\u001b[39m1\u001b[39m]:\n\u001b[0;32m     76\u001b[0m         \u001b[39m# emulate Py3K's behavior of telling us when an exception\u001b[39;00m\n\u001b[0;32m     77\u001b[0m         \u001b[39m# occurs in an exception handler.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\util\\compat.py:208\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    205\u001b[0m     exception\u001b[39m.\u001b[39m__cause__ \u001b[39m=\u001b[39m replace_context\n\u001b[0;32m    207\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n\u001b[0;32m    209\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[39m# credit to\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[39m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[39m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[39mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:661\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarttime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 661\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbapi_connection \u001b[39m=\u001b[39m connection \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39m_invoke_creator(\u001b[39mself\u001b[39m)\n\u001b[0;32m    662\u001b[0m     pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreated new connection \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, connection)\n\u001b[0;32m    663\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfresh \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:590\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[1;34m(connection_record)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    589\u001b[0m             \u001b[39mreturn\u001b[39;00m connection\n\u001b[1;32m--> 590\u001b[0m \u001b[39mreturn\u001b[39;00m dialect\u001b[39m.\u001b[39mconnect(\u001b[39m*\u001b[39mcargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams)\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:597\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[1;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams):\n\u001b[0;32m    596\u001b[0m     \u001b[39m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbapi\u001b[39m.\u001b[39mconnect(\u001b[39m*\u001b[39mcargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams)\n",
      "File \u001b[1;32mc:\\Users\\allen\\anaconda3\\Lib\\site-packages\\psycopg2\\__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     kwasync[\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    121\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 122\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39mconnection_factory, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwasync)\n\u001b[0;32m    123\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     conn\u001b[39m.\u001b[39mcursor_factory \u001b[39m=\u001b[39m cursor_factory\n",
      "\u001b[1;31mOperationalError\u001b[0m: (psycopg2.OperationalError) \n(Background on this error at: https://sqlalche.me/e/14/e3q8)"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, Float, String, Text, DateTime, MetaData\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from geoalchemy2 import Geometry\n",
    "from sqlalchemy.schema import CreateTable\n",
    "\n",
    "engine = create_engine('postgresql://xcq:x@localhost/FinalProject')\n",
    "Base = declarative_base()\n",
    "\n",
    "class Three_11(Base):\n",
    "    __tablename__ = \"three_11s\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    zipcode = Column(Integer)\n",
    "    latitude = Column(Float)\n",
    "    longitude = Column(Float)\n",
    "    created_date = Column(DateTime)\n",
    "    geometry = Column(Geometry('POINT'))  \n",
    "    complaint_type = Column(String)\n",
    "    \n",
    "class Tree(Base):\n",
    "    __tablename__ = \"trees\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    tree_id = Column(String)\n",
    "    zipcode = Column(Integer)\n",
    "    latitude = Column(Float)\n",
    "    longitude = Column(Float)\n",
    "    status = Column(String)\n",
    "    health = Column(String)\n",
    "    spc_common = Column(String)\n",
    "    geometry = Column(Geometry('POINT')) \n",
    "    \n",
    "class Zipcode(Base):\n",
    "    __tablename__ = \"zipcodes\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    zipcode = Column(Integer)\n",
    "    area = Column(String)\n",
    "    state = Column(String)\n",
    "    county = Column(String)\n",
    "    geometry = Column(Geometry('POLYGON')) \n",
    "\n",
    "\n",
    "class Zillow(Base):\n",
    "    __tablename__ = \"zillows\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    zipcode = Column(Integer)\n",
    "    city = Column(String)\n",
    "    county = Column(String)\n",
    "    date_2023_08_31 = Column(Float) \n",
    "    date_2023_01_31 = Column(Float)\n",
    "    date_2023_09_30 = Column(Float)\n",
    "    \n",
    "Base.metadata.drop_all(engine)\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = MetaData(bind=engine)\n",
    "metadata.reflect()\n",
    "\n",
    "with open('schema.sql', 'w') as f:\n",
    "    for table in Base.metadata.tables.values():\n",
    "        f.write(str(CreateTable(table)))\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_geodataframe_to_sql(gdf, table_name, engine, if_exists='replace', index=False):\n",
    "    \"\"\"\n",
    "    Writes a GeoDataFrame to a PostgreSQL database table.\n",
    "\n",
    "    Parameters:\n",
    "    gdf (GeoDataFrame): The GeoDataFrame to be written to the database.\n",
    "    table_name (str): Name of the target database table.\n",
    "    engine: SQLAlchemy database engine.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert geometry to WKT format, keep None if geometry is missing which is not possible since I drop N/A at the beginning.\n",
    "    gdf['geometry'] = gdf['geometry'].apply(lambda geom: geom.wkt if geom is not None else None)\n",
    "    gdf.to_sql(table_name, engine, if_exists=if_exists, index=index)\n",
    "\n",
    "\n",
    "def write_dataframe_to_sql(df, table_name, engine, if_exists='replace', index=False):\n",
    "    \"\"\"\n",
    "    Writes a DataFrame to a PostgreSQL database table.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame to be written to the database.\n",
    "    table_name (str): Name of the target database table.\n",
    "    engine: SQLAlchemy database engine.\n",
    "    \"\"\"\n",
    "\n",
    "    df.to_sql(table_name, engine, if_exists=if_exists, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_geodataframe_to_sql(cleaned_tree_gdf, 'trees', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/vwgvzrmn3kb9gnqyl0268qfh0000gn/T/ipykernel_31889/3595389091.py:12: UserWarning: Geometry column does not contain geometry.\n",
      "  gdf['geometry'] = gdf['geometry'].apply(lambda geom: geom.wkt if geom is not None else None)\n"
     ]
    }
   ],
   "source": [
    "write_geodataframe_to_sql(cleaned_311_gdf, 'three_11s', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_geodataframe_to_sql(cleaned_zipcode_gdf, 'zipcodes', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframe_to_sql(cleaned_zillow_df,'zillows',engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
